import streamlit as st
from google import genai
from app_around_leisure_restaurant import around_restaurant
import pandas as pd

LLM_MODEL = "gemini-2.5-flash"

def looks_like_food_request(text: str) -> bool:
    t = text.lower()
    food_keywords = ['ì¹˜í‚¨', 'í•œì‹', 'ì¤‘ì‹', 'ì§œì¥', 'ì§¬ë½•', 'í”¼ì', 'ì´ˆë°¥', 'ì¼ì‹',
                     'ì¡±ë°œ', 'ë³´ìŒˆ', 'ì¹¼êµ­ìˆ˜', 'ë¶„ì‹', 'í–„ë²„ê±°', 'ì¹´í˜', 'ì»¤í”¼', 'ì‹ë‹¹',
                     'ë§›ì§‘', 'ì–‘ì‹', 'íŒŒìŠ¤íƒ€', 'ë””ì €íŠ¸', 'ë² ì´ì»¤ë¦¬', 'ë¹µì§‘']
    if 'ì¶”ì²œ' in t or 'ë¨¹ê³ ' in t or 'ë¨¹ê³ ì‹¶' in t or 'ì•Œë ¤ì¤˜' in t:
        return True
    for kw in food_keywords:
        if kw in t:
            return True
    return False

def _get_client():
    api_key = st.secrets.get("GEMINI_API_KEY_mj")
    if not api_key:
        return None
    return genai.Client(api_key=api_key)

def _generate_reply(client, model, prompt):
    try:
        response = client.models.generate_content(
            model=model,
            contents=prompt,
        )
        return getattr(response, "text", str(response))
    except Exception as e:
        print(f"Error: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì  ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

# Gemini API ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
def stream_gemini_response(client, model, prompt):
    response_stream = client.models.generate_content_stream(model=model, contents=prompt)
    full_response = ""
    placeholder = st.empty()  # í…ìŠ¤íŠ¸ ë³´ì—¬ì¤„ ìë¦¬
    for chunk in response_stream:
        if chunk.text is not None:
            full_response += chunk.text
        placeholder.markdown(full_response)  # ë¶€ë¶„ í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
    return full_response

# ì¶”ì²œ í•¨ìˆ˜
def generate_food_recommendation(client, user_input, user_loc):
    latlon = (user_loc[0], user_loc[1])
    df_rec = around_restaurant(latlon)

    if df_rec is None or df_rec.empty:
        return 'í•´ë‹¹ ìœ„ì¹˜ ì£¼ë³€ì—ì„œ ì¶”ì²œí•  ë§Œí•œ ì‹ë‹¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'

    lines = []
    for i, row in df_rec.head(20).iterrows():
        name = row.get('ìƒí˜¸', '')
        addr = row.get('ë„ë¡œëª… ì£¼ì†Œ', '')
        dist = row.get('ê±°ë¦¬(km)')
        dist_text = f"{dist:.2f}km" if pd.notna(dist) else ''
        lines.append(f"[{i+1}] {name} / {addr} / {dist_text}")

    context_text = "\n".join(lines)

    system_instruction = (
        "ì œê³µëœ ì‹ë‹¹ ì¶”ì²œ ëª©ë¡ ì •ë³´ ìœ„ì£¼ë¡œ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì¤‘ë³µëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ í•œë²ˆë§Œ ì¶œë ¥ í•´ì£¼ì„¸ìš”\n"
        "ì¶”ì²œ ëª©ë¡ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì¶”ì²œ ëª©ë¡ì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.\n"
        "ì‚¬ìš©ìê°€ ìŒì‹ì´ë¦„ì„ ì…ë ¥í–ˆì„ ë•Œ ì‹ë‹¹ëª…ê³¼ ì‹ë‹¹ ì„¤ëª…ì„ ì´ìš©í•´ì„œ ê°™ì€ ìŒì‹ ì´ë¦„ì´ ë“¤ì–´ê°„ ê³³ ìœ„ì£¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n"
        "ì‹ë‹¹ì˜ ì£¼ì†ŒëŠ” ì‹ë‹¹ ëª©ë¡ì—ì„œ ì°¾ì•„ì„œ ì•Œë ¤ì£¼ì„¸ìš”.\n"
        "ì‹ë‹¹ì˜ ì£¼ì†Œë¥¼ ë¬¼ì–´ ë´¤ì„ ë•Œ, ëª©ë¡ì—ì„œ ì°¾ê±°ë‚˜ ì¸í„°ë„· ê²€ìƒ‰ í›„ ì•Œë ¤ë“œë¦´ì§€ ë¬¼ì–´ë´ ì£¼ì„¸ìš”.\n"
        "ëª©ë¡ì— ì—†ëŠ” ì‹ë‹¹ì´ë‚˜ ìŒì‹ì´ ê²€ìƒ‰ë˜ë©´ ì¸í„°ë„·ìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰ ì—¬ë¶€ë¥¼ ë¬¸ì˜í•˜ì„¸ìš”.\n"
        "ì‚¬ìš©ì ì§ˆë¬¸ì´ ëë‚˜ê¸° ì „ì— ë” ê¶ê¸ˆí•œ ì ì´ ìˆëŠ”ì§€ ë‹¤ì‹œ ì§ˆë¬¸ ìœ ë„í•˜ì„¸ìš”."
        "ê²€ìƒ‰ì´ ëŠ¦ì–´ì§„ë‹¤ë©´ 'ê²€ìƒ‰ì´ ëŠ¦ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì¤˜.\n "
        "ê±´ë¬¼ ì£¼ì†Œë‚˜ ì´ë¦„ì„ ì…ë ¥í•˜ë©´ 'ê±´ë¬¼ ìœ„ì£¼ ê²€ìƒ‰ì€ ì•„ì§ ì¤€ë¹„ ì¤‘ ì…ë‹ˆë‹¤'ë¼ê³  ë§í•´ì£¼ì„¸ìš”.\n"
    )
    prompt = system_instruction + "\nì‹ë‹¹ ëª©ë¡:\n" + context_text + "\n\nì§ˆë¬¸: " + user_input

    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
    reply = stream_gemini_response(st.session_state.client, LLM_MODEL, prompt)

    return reply

# ì¼ë°˜ ë‹µë³€ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì ìš© ì—†ìŒ)
def stream_general_reply(client, messages):
    system_instruction = (
        "ë‹¹ì‹ ì€ ì–´ë¥´ì‹ (ë…¸ë…„ì¸µ)ì„ ëŒ€ìƒìœ¼ë¡œ ìƒëƒ¥í•˜ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì‘ë‹µí•˜ëŠ” ìƒë‹´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
        "ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ì²œì²œíˆ, ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”. ì–´ë ¤ìš´ ìš©ì–´ëŠ” ì‰¬ìš´ ë§ë¡œ í’€ì–´ ì„¤ëª…í•˜ê³ , "
        "í•œ ë²ˆì— í•œ ê°€ì§€ ì •ë³´ë¥¼ ì œê³µí•˜ë©° ë°°ë ¤ì‹¬ ìˆê³  ê³µì†í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        "ë‹µë³€í• ë•Œ ì–´ë¥´ì‹  ë³´ë‹¤ëŠ” ì‚¬ìš©ìë‹˜ ì„ ì“°ë˜ ì¹œê·¼í•˜ê²Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”."
        "ê²€ìƒ‰ ì§€ì—­ì€ 'ì¸ì²œ' ìœ¼ë¡œ í•œì •í•©ë‹ˆë‹¤"
        "ëª¨ë¥´ëŠ”ê±´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”"
        "ì‹ë‹¹ì˜ ì£¼ì†Œë¥¼ ì‹ë‹¹ ëª©ë¡ì—ì„œ ì°¾ê³  ì¸í„°ë„·ì—ì„œ ê²€ìƒ‰í•´ì„œ ë‹¬ë¼ì§„ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”."
        "ì¶”ì²œ ëª©ë¡ì— ì—†ëŠ” ì‹ë‹¹ì´ë‚˜ ìŒì‹ì„ ë¬¼ì–´ë³¼ë•ŒëŠ” ìµœê·¼ ê²€ìƒ‰í•œ ì‹ë‹¹ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."
        "ê±´ë¬¼ ì£¼ì†Œë‚˜ ì´ë¦„ì„ ì…ë ¥í•˜ë©´ 'ê±´ë¬¼ ìœ„ì£¼ ê²€ìƒ‰ì€ ì•„ì§ ì¤€ë¹„ ì¤‘ ì…ë‹ˆë‹¤'ë¼ê³  ë§í•´ì£¼ì„¸ìš”."
    )
    conversation_text = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
    prompt = system_instruction + "\n\n" + conversation_text

    response_stream = client.models.generate_content_stream(model=LLM_MODEL, contents=prompt)

    full_text = ""
    placeholder = st.empty()
    for chunk in response_stream:
        full_text += chunk.text
        placeholder.markdown(full_text)

    return full_text

def run_chatbot_app():
    st.markdown(
    "<h3 style='color: orange;'>ì‚¬ìš©ì ê·¼ì²˜ì˜ ì‹ë‹¹ì„ ì¶”ì²œí•´ì£¼ëŠ” AIğŸ’»</h3>", 
    unsafe_allow_html=True
)
    st.text("ğŸ“ ì°¾ì•„ë³´ê³  ì‹¶ì€ ì‹ë‹¹ ë˜ëŠ” ìŒì‹ì„ ë§ì”€í•´ì£¼ì„¸ìš” : ì˜ˆ) ì—¬ê¸° ê·¼ì²˜ ì¤‘ì‹ë‹¹ ì°¾ì•„ì¤˜")

    # ìœ„ì¹˜ ë³€ê²½ ì‹œ ë©”ì‹œì§€ ì´ˆê¸°í™”
    prev_loc = st.session_state.get("prev_user_location")
    current_loc = st.session_state.get("user_location")
    if current_loc != prev_loc:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
        ]
        st.session_state["prev_user_location"] = current_loc

    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = _get_client()
    if not client:
        st.error("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    st.session_state.client = client  # ê¸€ë¡œë²Œ ì €ì¥ (í•„ìš”ì‹œ)

    # UI ì¶œë ¥
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.chat_message("user").write(msg["content"])
        else:
            st.chat_message("assistant").write(msg["content"])

    # ìœ„ì¹˜ ë¯¸ì…ë ¥ ì‹œ ê²½ê³ 
    if not st.session_state.get("user_location"):
        st.warning("2ë²ˆ íŒŒì¼ì—ì„œ ìœ„ì¹˜ë¥¼ ë¨¼ì € ì…ë ¥í•˜ì„¸ìš”.")

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    with st.expander('ê²€ìƒ‰ì°½ ì…ë‹ˆë‹¤', expanded=True):
        user_input = st.chat_input("=>ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”")
        if user_input and user_input.strip() != "":
            st.session_state.messages.append({"role": "user", "content": user_input})
            if looks_like_food_request(user_input):
                user_loc = st.session_state.get('user_location')
                if not user_loc:
                    st.session_state.messages.append({"role": "assistant", "content": "ë¨¼ì € ìœ„ì¹˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”."})
                    return
                reply = generate_food_recommendation(st.session_state.client, user_input, user_loc)
                st.session_state.messages.append({"role": "assistant", "content": reply})
            else:
                reply = stream_general_reply(st.session_state.client, st.session_state.messages)
                st.session_state.messages.append({"role": "assistant", "content": reply})

if __name__ == '__main__':
    run_chatbot_app()
