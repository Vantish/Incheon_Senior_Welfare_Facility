import os
import streamlit as st
from pathlib import Path

# (!!!) [ì¶”ê°€] ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§ì ‘ ì„í¬íŠ¸í•©ë‹ˆë‹¤ (ì§„ë‹¨ìš©)
import chromadb

from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

CHROMA_DIR = './chroma_db'
EMBED_MODEL = "text-embedding-004" 
LLM_MODEL = "gemini-2.5-flash" 

# --- [ì‹œì‘] Streamlit ì•± ì„¤ì • ---
st.set_page_config(
    page_title="ì–´ë¥´ì‹  ë³µì§€ ì±—ë´‡",
    page_icon="ğŸ‘µ",
    layout="centered",
)
st.title("ğŸ‘µ ì–´ë¥´ì‹  ê±´ê°•ë³µì§€ ì±—ë´‡")

# --- [ìˆ˜ì •] API í‚¤ ë¡œë“œ ë° OS í™˜ê²½ë³€ìˆ˜ ì„¤ì • ---
# secrets.tomlì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
api_key = st.secrets.get("GOOGLE_API_KEY")

if api_key:
    # LangChain ë° Google ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì½ì„ ìˆ˜ ìˆë„ë¡ OS í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •
    os.environ["GOOGLE_API_KEY"] = api_key
else:
    # API í‚¤ê°€ ì—†ëŠ” ê²½ìš° ì‚¬ìš©ìì—ê²Œ ëª…í™•íˆ ì•Œë¦¬ê³  ì•± ì¤‘ì§€
    st.error("Google API í‚¤ë¥¼ .streamlit/secrets.toml íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()


# --- [ìˆ˜ì •] í—¬í¼ í•¨ìˆ˜ (Pickle ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ Top-levelë¡œ ì´ë™) ---

def format_docs(docs):
    """
    ê²€ìƒ‰ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€
    ë‹¨ì¼ ë¬¸ìì—´(context)ê³¼ ì¶œì²˜ ë¬¸ìì—´(source)ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    """
    context_parts = []
    source_names = set() # ì¤‘ë³µ ì¶œì²˜ ì œê±°ìš©
    
    for i, doc in enumerate(docs, 1):
        # page_content í¬ë§·íŒ… (ë‚´ìš©)
        content = doc.page_content.strip()
        context_parts.append(f"[{i}] {content}")
        
        # metadata í¬ë§·íŒ… (ì¶œì²˜)
        source = doc.metadata.get("source", "N/A")
        # íŒŒì¼ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ì˜ˆ: ./data/file.pdf -> file.pdf)
        source_name = Path(source).name
        source_names.add(source_name)

    # ìµœì¢… ë¬¸ìì—´ ìƒì„±
    context_str = "\n\n".join(context_parts)
    source_str = ", ".join(source_names) # ì¶œì²˜ íŒŒì¼ëª…ë“¤ì„ ì½¤ë§ˆë¡œ ì—°ê²°
    
    # contextì™€ sourceë¥¼ íŠœí”Œë¡œ ë°˜í™˜
    return (context_str, source_str)

def add_source_to_answer(result):
    """
    LLMì˜ ë‹µë³€(answer)ê³¼ í¬ë§·íŒ…ëœ ì¶œì²˜(source)ë¥¼ ê²°í•©í•˜ì—¬
    ìµœì¢… ì‚¬ìš©ì ë‹µë³€ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    answer = result["answer"]
    source = result["source"]
    
    if source and source != "N/A":
        return f"{answer}\n\n---\n**ì¶œì²˜:** {source}"
    else:
        return answer

# -----------------------------
# 1. DB ë¡œë“œ ì „ìš© í•¨ìˆ˜ (ìºì‹± ì ìš© ë° ì§„ë‹¨ ì½”ë“œ ê°•í™”)
# -----------------------------
@st.cache_resource
def load_vectorstore():
    """
    Streamlit ì•± ì‹¤í–‰ ì‹œ ë‹¨ í•œ ë²ˆë§Œ ChromaDBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    (!!!) DB ì†ìƒ ì—¬ë¶€ë¥¼ ì§„ë‹¨í•˜ëŠ” ì½”ë“œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (!!!)
    """
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBED_MODEL)
    db_path = Path(CHROMA_DIR)

    if not db_path.exists() or not (db_path / "chroma.sqlite3").exists():
        st.error(f"'{CHROMA_DIR}' í´ë” ë˜ëŠ” 'chroma.sqlite3' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.error("Colabì—ì„œ 'chroma_db'ë¥¼ ë¹Œë“œí•œ í›„, ì••ì¶• í•´ì œí•˜ì—¬ VScode í”„ë¡œì íŠ¸ í´ë”ì— ì˜¬ë°”ë¥´ê²Œ ë³µì‚¬í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    try:
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ì— ì§ì ‘ ì—°ê²°í•˜ì—¬ ì§„ë‹¨ ì‹œì‘
        client = chromadb.PersistentClient(path=CHROMA_DIR)
        
        # 1. ì»¬ë ‰ì…˜ ëª©ë¡ í™•ì¸
        collections = client.list_collections()
        if not collections:
            st.error(f"'{CHROMA_DIR}' DBëŠ” ë¡œë“œë˜ì—ˆìœ¼ë‚˜, ì•ˆì— ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.error("Colab DB ë¹Œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ìˆì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Colabì—ì„œ DBë¥¼ ë‹¤ì‹œ ë¹Œë“œí•˜ì„¸ìš”.")
            st.stop()

        # 2. 'langchain' (ê¸°ë³¸ê°’) ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        # (Chroma.from_documentsëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 'langchain' ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤)
        try:
            collection = client.get_collection(name="langchain")
        except Exception as e:
            st.error(f"DBì—ì„œ 'langchain' ì»¬ë ‰ì…˜ì„ ì°¾ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
            st.error(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {[c.name for c in collections]}")
            st.error("Colabì˜ chromadb ë²„ì „(1.3.0)ê³¼ ë¡œì»¬ VScodeì˜ chromadb ë²„ì „(1.3.0)ì´ ë™ì¼í•œì§€ í™•ì¸í•˜ì„¸ìš”.")
            st.stop()

        # 3. ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
        count = collection.count()
        if count == 0:
            st.warning(f"'{CHROMA_DIR}' DBëŠ” ë¡œë“œë˜ì—ˆìœ¼ë‚˜, 'langchain' ì»¬ë ‰ì…˜ ì•ˆì— ë¬¸ì„œê°€ 0ê°œì…ë‹ˆë‹¤.")
            st.warning("Colabì—ì„œ DBê°€ ì •ìƒì ìœ¼ë¡œ ë¹Œë“œë˜ì—ˆëŠ”ì§€, 'chroma_db' í´ë”ê°€ ì˜¬ë°”ë¥´ê²Œ ë³µì‚¬/ì••ì¶• í•´ì œë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
            st.stop()
        
        # í„°ë¯¸ë„(ì½˜ì†”)ì— ì„±ê³µ ë¡œê·¸ ì¶œë ¥
        print(f"\n--- [DB ì§„ë‹¨ ì„±ê³µ] ---")
        print(f"'{CHROMA_DIR}' DB ë¡œë“œ ì„±ê³µ.")
        print(f"ì»¬ë ‰ì…˜ '{collection.name}'ì—ì„œ {count}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        print(f"----------------------\n")

        # 4. LangChain VectorStore ê°ì²´ë¡œ ë˜í•‘
        # (ì§„ë‹¨ì´ ëë‚¬ìœ¼ë¯€ë¡œ, LangChainì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°ì²´ë¥¼ ìƒì„±)
        vectordb = Chroma(
            client=client,
            collection_name="langchain",
            embedding_function=embeddings,
        )
        return vectordb

    except Exception as e:
        st.error(f"DB ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("ChromaDB íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Colabì—ì„œ DBë¥¼ ë‹¤ì‹œ ë¹Œë“œí•˜ê³  VScodeì˜ `chromadb` ë²„ì „ì„ (1.3.0) í†µì¼í•˜ì„¸ìš”.")
        st.stop()


# -----------------------------
# 2. RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜ (ìºì‹± ì ìš©)
# -----------------------------
@st.cache_resource
def make_rag_chain(_vectordb): # _vectordb ì¸ìë¥¼ ë°›ì•„ ìºì‹œê°€ ì¸ì‹í•˜ë„ë¡ í•¨
    """
    DB ë¡œë”(retriever)ì™€ LLMì„ ë¬¶ì–´ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # (!!!) [ìµœì¢… ìˆ˜ì •] 
    # 1. k=10 ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥
    # 2. search_type="mmr" ë¡œ ì„¤ì •í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ì˜ 'ë‹¤ì–‘ì„±' í™•ë³´
    retriever = _vectordb.as_retriever(
        search_type="mmr", 
        search_kwargs={"k": 10}
    )

    system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ë‹µí•˜ëŠ” ë…¸ì¸ ê±´ê°• ë° ë³µì§€ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê·œì¹™ì„ ì§€í‚¤ì„¸ìš”:
- ê°€ëŠ¥í•œ í•œ ì œê³µëœ ê·¼ê±°(ë§¥ë½)ì— ê¸°ë°˜í•˜ì—¬ ê°„ê²°/ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
- ìˆ«ì/ì œë„ëª… ë“±ì€ ì›ë¬¸ í‘œí˜„ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ì„¸ìš”.
- ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. (ì¶”ì¸¡ ê¸ˆì§€)
- ë‹¹ì‹ ì´ ìƒëŒ€í•˜ëŠ” ì‚¬ìš©ìëŠ” ì—°ë ¹ëŒ€ê°€ ë†’ì€ ë…¸ì¸ë¶„ë“¤ì´ë‚˜ ë…¸ì¸ë³µì§€ ì¢…ì‚¬ìì´ë‹ˆ ìµœëŒ€í•œ ìƒëƒ¥í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì„¸ìš”.
- ì‚¬ìš©ìë¥¼ í˜¸ì¹­í•  ë•Œ 'ì‚¬ìš©ìë‹˜' ì´ë¼ê³  ì •ì¤‘í•˜ê²Œ ë¶€ë¥´ì„¸ìš”.
- ì‚¬ìš©ìê°€ ê±´ê°•ì´ë‚˜ ë³µì§€ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹Œ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•  ê²½ìš° ë¶€ë“œëŸ½ê²Œ ê±°ì ˆí•˜ì„¸ìš”.
"""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "ì§ˆë¬¸: {question}\n\në‹¤ìŒì€ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°ì…ë‹ˆë‹¤:\n{context}"),
        ]
    )

    llm = ChatGoogleGenerativeAI(
        model=LLM_MODEL,
        temperature=0.2, # ë‹µë³€ì˜ ì°½ì˜ì„± ì¡°ì ˆ (ë‚®ì„ìˆ˜ë¡ ì‚¬ì‹¤ ê¸°ë°˜)
    )

    # --- [ìˆ˜ì •] LCEL ì²´ì¸ (Pickle, | ì—°ì‚°ì ì˜¤ë¥˜ í•´ê²°) ---

    # 1. retrieverê°€ ì§ˆë¬¸(question)ì„ ë°›ì•„ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
    #    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ format_docs í•¨ìˆ˜ë¡œ í¬ë§·íŒ…í•˜ì—¬ (context, source) íŠœí”Œ ìƒì„±
    retrieval_chain = RunnableLambda(
        lambda x: format_docs(retriever.invoke(x["question"]))
    )
    
    # 2. (context, source) íŠœí”Œì„ ë°›ì•„ LLM í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ í˜•ì‹ìœ¼ë¡œ ë§¤í•‘
    formatting_chain = RunnableLambda(
        lambda tup: {"context": tup[0], "source": tup[1], "question": tup[2]}
    )

    # 3. LLMì´ (context, question)ì„ ë°›ì•„ ë‹µë³€(answer) ìƒì„±
    prompt_chain = {
        "answer": prompt | llm | StrOutputParser(),
        "source": lambda x: x["source"] # sourceëŠ” ê·¸ëŒ€ë¡œ í†µê³¼
    }
    
    # 4. ìµœì¢…ì ìœ¼ë¡œ (answer, source)ë¥¼ ë°›ì•„ add_source_to_answerë¡œ í•©ì¹¨
    final_chain = (
        {
            "result": retrieval_chain, # (context, source) íŠœí”Œ ìƒì„±
            "question": lambda x: x["question"] # ì›ë³¸ ì§ˆë¬¸ í†µê³¼
        }
        | RunnableLambda(lambda x: formatting_chain.invoke((x["result"][0], x["result"][1], x["question"])))
        | prompt_chain
        | RunnableLambda(add_source_to_answer)
    )
    
    return final_chain


def run_chatbot():
    """ë©”ì¸ ì±—ë´‡ ì‹¤í–‰ ë¡œì§"""

    # --- [ìˆ˜ì •] ì•± ì‹œì‘ ì‹œ ë¦¬ì†ŒìŠ¤ ë¡œë“œ (ìºì‹œ ì‚¬ìš©) ---
    try:
        # ìºì‹œëœ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ DBì™€ ì²´ì¸ì„ ë¡œë“œ
        vectordb = load_vectorstore()
        chain = make_rag_chain(vectordb)
    except Exception as e:
        # ë¡œë”© ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ë‹¨
        st.error(f"ì±—ë´‡ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    # --- [ìˆ˜ì •] ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™” ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # --- [ìˆ˜ì •] ì±„íŒ… ê¸°ë¡ í‘œì‹œ ---
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- [ìˆ˜ì •] ì‚¬ìš©ì ì…ë ¥ ë° ì±—ë´‡ ì‘ë‹µ ì²˜ë¦¬ ---
    user_question = st.chat_input("ê¶ê¸ˆí•˜ì‹  ì ì„ ë§ì”€í•´ ì£¼ì„¸ìš” ! ")
    
    if user_question:
        # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        # 2. ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # RAG ì²´ì¸ í˜¸ì¶œ
                    answer = chain.invoke({"question": user_question})
                except Exception as e:
                    answer = f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

                # 3. ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.markdown(answer)

if __name__ == '__main__':
    run_chatbot()

